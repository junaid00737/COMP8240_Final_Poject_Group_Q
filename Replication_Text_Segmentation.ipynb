{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Junaid Ur Rehman\\Documents\\Master of Business Analytics\\Semester 5\\Applications of Data Science\\Final Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain, tee\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "\n",
    "# Set current working directory if needed\n",
    "# os.chdir(r\"C:\\Users\\Junaid Ur Rehman\\Documents\\Master of Business Analytics\\Semester 5\\Applications of Data Science\\Final Project\")\n",
    "\n",
    "# Load stopwords\n",
    "stopword_set = set()\n",
    "# stopword_file_path = \"Github Code/data/STOPWORD.list\"\n",
    "stopword_file_path = \"Github Code/data/STOPWORD.list\"\n",
    "\n",
    "if os.path.exists(stopword_file_path):\n",
    "    with open(stopword_file_path) as f:\n",
    "        for line in f:\n",
    "            stopword_set.add(line.strip())\n",
    "\n",
    "# Add punctuation to stopwords\n",
    "stopword_set.update([\"''\", \",\", \".\", \"``\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\",\n",
    "                     \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"])\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "CHOI_TEMPLATE = \"./Choi Dataset/naacl00Exp/data/{}/{}/{}.ref\"\n",
    "\n",
    "def choi_loader(doc, tp, ref, word_cut=0, remove_stop=False, stem=False):\n",
    "    \"\"\" Load a choi document from the dataset,\n",
    "        returns a list of parts\n",
    "        each part is a list of sentences\n",
    "        each sentence is a list of words,\n",
    "        the only preprocessing is to lowercase everything \"\"\"\n",
    "    try:\n",
    "        with open(CHOI_TEMPLATE.format(doc, tp, ref)) as f:\n",
    "            doc = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File {CHOI_TEMPLATE.format(doc, tp, ref)} not found. Please check the file paths.\")\n",
    "\n",
    "    def is_valid(word):\n",
    "        if stem:\n",
    "            word = stemmer.stem(word)\n",
    "        if remove_stop:\n",
    "            if word in stopword_set:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    parts = [x.splitlines() for x in doc.split(\"==========\\n\") if x]\n",
    "\n",
    "    doc = [[[x.lower() for x in sent.split() if is_valid(x)] for sent in doc] for doc in parts]\n",
    "\n",
    "    filtered = [[sent for sent in part if len(sent) > word_cut] for part in doc]\n",
    "\n",
    "    return filtered\n",
    "\n",
    "ARX_TEMPLATE = \"./Github Code/data/arxiv/{:03d}.ref\"\n",
    "\n",
    "# def arx_loader(num):\n",
    "#     \"\"\" Load an arxiv document from the dataset,\n",
    "#         returns a list of parts\n",
    "#         each part is a list of words \"\"\"\n",
    "#     try:\n",
    "#         with open(ARX_TEMPLATE.format(num)) as f:\n",
    "#             doc = f.read()\n",
    "#     except FileNotFoundError:\n",
    "#         raise FileNotFoundError(f\"File {ARX_TEMPLATE.format(num)} not found. Please check the file paths.\")\n",
    "\n",
    "#     return [[[x] for x in x.split()] for x in doc.split(\"BR\")]\n",
    "\n",
    "def arx_loader(num):\n",
    "    \"\"\" Load an arxiv document from the dataset,\n",
    "        returns a list of parts\n",
    "        each part is a list of words \"\"\"\n",
    "    try:\n",
    "        # Use errors=\"ignore\" to skip invalid characters\n",
    "        with open(ARX_TEMPLATE.format(num), encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            doc = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File {ARX_TEMPLATE.format(num)} not found. Please check the file paths.\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise Exception(f\"Error decoding file {ARX_TEMPLATE.format(num)}: {e}. Ensure the file is in UTF-8 format.\")\n",
    "\n",
    "    return [[[x] for x in x.split()] for x in doc.split(\"BR\")]\n",
    "\n",
    "def allchoi(set, *args, **kwargs):\n",
    "    if set == \"3-5\":\n",
    "        for a in [1, 2]:\n",
    "            for i in range(50):\n",
    "                yield choi_loader(a, \"3-5\", i, *args, **kwargs)\n",
    "    elif set == \"6-8\":\n",
    "        for a in [1, 2]:\n",
    "            for i in range(50):\n",
    "                yield choi_loader(a, \"6-8\", i, *args, **kwargs)\n",
    "    elif set == \"9-11\":\n",
    "        for a in [1, 2]:\n",
    "            for i in range(50):\n",
    "                yield choi_loader(a, \"9-11\", i, *args, **kwargs)\n",
    "    elif set == \"3-11\":\n",
    "        for a in [1, 2]:\n",
    "            for i in range(50):\n",
    "                yield choi_loader(a, \"3-11\", i, *args, **kwargs)\n",
    "        for i in range(300):\n",
    "            yield choi_loader(3, \"3-11\", i, *args, **kwargs)\n",
    "\n",
    "def collapse(doc):\n",
    "    \"\"\" Turn a document into a single string \"\"\"\n",
    "    return \" \".join(\" \".join(\" \".join(sent) for sent in part) for part in doc)\n",
    "\n",
    "def collapse_sents(doc):\n",
    "    \"\"\" Collapse a doc to a list of sentences \"\"\"\n",
    "    return [sent for part in doc for sent in part]\n",
    "\n",
    "def collapse_words(doc):\n",
    "    \"\"\" Collapse a doc to a list of words \"\"\"\n",
    "    return [word for part in doc for sent in part for word in sent]\n",
    "\n",
    "def word_iter(doc):\n",
    "    \"\"\" Iterate over the words in a document \"\"\"\n",
    "    words = (word for part in doc for sent in part for word in sent)\n",
    "    for word in words:\n",
    "        yield word\n",
    "\n",
    "def sent_iter(doc):\n",
    "    \"\"\" Iterate over the sentences in a document \"\"\"\n",
    "    sents = (sent for part in doc for sent in part)\n",
    "    for sent in sents:\n",
    "        yield sent\n",
    "\n",
    "def refsplit(doc):\n",
    "    \"\"\" Get the reference splitting for the document \"\"\"\n",
    "    middle = np.cumsum([1] + [sum(1 for sent in part for word in sent) for part in doc])\n",
    "    return (middle[1:-1] - 1).tolist() + [middle[-1] - 1]\n",
    "\n",
    "def refsplit_sent(doc):\n",
    "    \"\"\" Get the reference splitting for the sentence representation \"\"\"\n",
    "    middle = np.cumsum([1] + [sum(1 for sent in part) for part in doc])\n",
    "    return (middle[1:-1] - 1).tolist() + [middle[-1] - 1]\n",
    "\n",
    "# A testing document in the same structure as a choi doc\n",
    "testdoc = [[\"this is the first sentence\".split()] * 5,\n",
    "           [\"second sentence same as the first\".split()] * 3,\n",
    "           [\"the blue fish went to the market\".split()] * 4,\n",
    "           [\"once upon a midnight dreary with my pack\".split()] * 5,\n",
    "           [\"while i pondered weak and weary\".split()] * 3,\n",
    "           [\"one fish two fish three fish blue fish\".split()] * 5,\n",
    "           [\"pack it up pack it in let me begin\".split()] * 3,\n",
    "           [\"i came to win battle me that is a sin to begin\".split()] * 5,\n",
    "           [\"and think about how ravens and writing desks\".split()] * 4,\n",
    "           [\"other people are people too not ravens or fish\".split()] * 3]\n",
    "\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def seg_iter(splits):\n",
    "    return pairwise([0] + splits)\n",
    "\n",
    "def length_iter(splits):\n",
    "    return ((b - a) for (a, b) in seg_iter(splits))\n",
    "\n",
    "xnor = lambda a, b: (a and b) or (not a and not b)\n",
    "\n",
    "def score(hyp, ref, k=None):\n",
    "    k = k or int(round(0.5 * ref[-1] / len(ref))) - 1\n",
    "\n",
    "    length = ref[-1]\n",
    "    probeinds = np.arange(length - k)\n",
    "    dref = np.digitize(probeinds, ref) == np.digitize(probeinds + k, ref)\n",
    "    dhyp = np.digitize(probeinds, hyp) == np.digitize(probeinds + k, hyp)\n",
    "\n",
    "    return (dref ^ dhyp).mean()\n",
    "\n",
    "def score_wd(hyp, ref, k=None):\n",
    "    k = k or int(round(0.5 * ref[-1] / len(ref))) - 1\n",
    "\n",
    "    length = ref[-1]\n",
    "    hyp = np.asarray(hyp)\n",
    "    ref = np.asarray(ref)\n",
    "\n",
    "    score = 0.0\n",
    "    tot = 0.0\n",
    "    for i in range(length - k):\n",
    "        bref = ((ref > i) & (ref <= i + k)).sum()\n",
    "        bhyp = ((hyp > i) & (hyp <= i + k)).sum()\n",
    "        score += 1.0 * (np.abs(bref - bhyp) > 0)\n",
    "        tot += 1.0\n",
    "    return score / tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOTmXVpXhaO0",
    "outputId": "d001e1f1-74da-4999-ca5e-697fdfac6234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Representation for Sentences:\n",
      " [[1. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code for figuring out various vector representations of documents\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def tf_sents(doc):\n",
    "    \"\"\" Create a sentence level tf representation of the document \"\"\"\n",
    "    words = set(word for word in word_iter(doc))\n",
    "    word_pk = {word: pk for pk, word in enumerate(words)}\n",
    "\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            wordcounter = defaultdict(int)\n",
    "            for word in sent:\n",
    "                wordcounter[word] += 1\n",
    "\n",
    "            vec = np.zeros(len(words))\n",
    "            for word, count in wordcounter.items():  # Changed iteritems() to items() for Python 3\n",
    "                if word in words:\n",
    "                    vec[word_pk[word]] += count\n",
    "            vecs.append(vec)\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def tf_words(doc):\n",
    "    \"\"\" Create a word level tf representation of the document \"\"\"\n",
    "    words = set(word for word in word_iter(doc))\n",
    "    word_pk = {word: pk for pk, word in enumerate(words)}\n",
    "\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            for word in sent:\n",
    "                vec = np.zeros(len(words))\n",
    "                if word in words:\n",
    "                    vec[word_pk[word]] += 1\n",
    "                vecs.append(vec)\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def vec_sents(doc, word_lookup, wordreps):\n",
    "    \"\"\" Create a vector representation of the document \"\"\"\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            wordvecs = [np.zeros(wordreps.shape[1])]\n",
    "            for word in sent:\n",
    "                pk = word_lookup.get(word, -1)\n",
    "                if pk >= 0:\n",
    "                    wordvecs.append(wordreps[pk])\n",
    "            vecs.append(np.mean(wordvecs, axis=0))\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def vec_words(doc, word_lookup, wordreps):\n",
    "    \"\"\" Create a vector representation of the document \"\"\"\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            for word in sent:\n",
    "                pk = word_lookup.get(word, -1)\n",
    "                if pk >= 0:\n",
    "                    vecs.append(wordreps[pk])\n",
    "                else:\n",
    "                    vecs.append(np.zeros(wordreps.shape[1]))\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def vectop_sents(doc, word_lookup, wordreps):\n",
    "    \"\"\" Create a vector representation of the document \"\"\"\n",
    "    vecs = []\n",
    "    N = wordreps.max() + 1\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            sentvec = np.zeros(N)\n",
    "            for word in sent:\n",
    "                pk = word_lookup.get(word, -1)\n",
    "                if pk >= 0:\n",
    "                    sentvec[wordreps[word_lookup[word]]] += 1\n",
    "            vecs.append(sentvec)\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def vecdf_sents(doc, word_lookup, wordreps, dfcounter):\n",
    "    \"\"\" Create a vector representation of the document \"\"\"\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            wordvecs = [np.zeros(wordreps.shape[1])]\n",
    "            for word in sent:\n",
    "                pk = word_lookup.get(word, -1)\n",
    "                if pk >= 0:\n",
    "                    wordvecs.append(np.log(500. / (dfcounter.get(word, 1.0) + 0.0)) * wordreps[pk])\n",
    "            vecs.append(np.mean(wordvecs, axis=0))\n",
    "\n",
    "    return np.array(vecs)\n",
    "\n",
    "def vecdf_words(doc, word_lookup, wordreps, dfcounter):\n",
    "    \"\"\" Create a vector representation of the document \"\"\"\n",
    "    vecs = []\n",
    "    for part in doc:\n",
    "        for sent in part:\n",
    "            for word in sent:\n",
    "                pk = word_lookup.get(word, -1)\n",
    "                if pk >= 0:\n",
    "                    vecs.append(np.log(500. / (dfcounter.get(word, 1.0) + 0.0)) * wordreps[pk])\n",
    "                else:\n",
    "                    vecs.append(np.zeros(wordreps.shape[1]))\n",
    "    return np.array(vecs)\n",
    "\n",
    "\n",
    "# Sample document (list of parts, each part is a list of sentences, each sentence is a list of words)\n",
    "test_doc = [[[\"this\", \"is\", \"a\", \"test\"], [\"another\", \"sentence\"]], [[\"more\", \"words\", \"here\"]]]\n",
    "\n",
    "# Testing tf_sents function\n",
    "tf_vector = tf_sents(test_doc)\n",
    "print(\"TF Representation for Sentences:\\n\", tf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "80qMSj0rhkqF"
   },
   "outputs": [],
   "source": [
    "\"\"\n",
    "import numpy as np\n",
    "from scipy.ndimage import generic_filter\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.random import rand\n",
    "\n",
    "####################\n",
    "# C99\n",
    "####################\n",
    "'''\n",
    "def rankkern(x):\n",
    "    \"\"\"The kernel for the rank transformation, measures the fraction of the neighbors that\n",
    "    take on a value less than the middle value.\"\"\"\n",
    "    n = x.size\n",
    "    mid = n // 2\n",
    "    better = ((x >= 0) & (x < x[mid])).sum()\n",
    "    return better / ((x >= 0).sum() - 1.0)'''\n",
    "    \n",
    "def rankkern(x):\n",
    "    \"\"\" The kernel for the rank transformation, measures the fraction of the neighbors that\n",
    "    take on a value less than the middle value \"\"\"\n",
    "    n = x.size\n",
    "    mid = n // 2\n",
    "    better = ((x >= 0) & (x < x[mid])).sum()\n",
    "    total_neighbors = (x >= 0).sum() - 1.0\n",
    "    return better / total_neighbors if total_neighbors != 0 else 0\n",
    "\n",
    "def rankify(mat, size=11):\n",
    "    \"\"\"Apply the ranking transformation of a given size.\"\"\"\n",
    "    return generic_filter(mat, rankkern, size=(size, size), mode='constant', cval=-1)\n",
    "    \n",
    "def c99score(distsmat, hyp, minlength=1, maxlength=None):\n",
    "    \"\"\"Do the Choi C99 scoring for a hypothesis splitting.\"\"\"\n",
    "    N = distsmat.shape[0]\n",
    "    beta = 0.0\n",
    "    alpha = 0.0\n",
    "    for (a, b) in seg_iter(hyp):\n",
    "        beta += distsmat[a:b, a:b].sum()\n",
    "        alpha += (b - a) ** 2\n",
    "        if minlength and (b - a) < minlength:\n",
    "            beta += -np.inf\n",
    "        if maxlength and (b - a) > maxlength:\n",
    "            beta += -np.inf\n",
    "    return -beta / (alpha + 0.0)\n",
    "\n",
    "def c99split(distsmat, k, rank=0, *args, **kwargs):\n",
    "    \"\"\"Do the Choi style C99 splitting, given a matrix of distances D,\n",
    "    and k splits to perform. The rank keyword denotes whether we want to\n",
    "    do the ranking transformation if positive and if so denotes the size of the\n",
    "    ranking filter.\"\"\"\n",
    "    if rank:\n",
    "        distsmat = rankify(distsmat, rank)\n",
    "\n",
    "    N = distsmat.shape[0]\n",
    "    splits = [N]\n",
    "    for n in range(k):\n",
    "        newans = min(\n",
    "            (c99score(distsmat, sorted(splits + [i]), *args, **kwargs), splits + [i])\n",
    "            for i in range(1, N - 1) if i not in set(splits)\n",
    "        )\n",
    "        splits = newans[1]\n",
    "    return sorted(splits), newans[0]\n",
    "\n",
    "####################\n",
    "# DP\n",
    "####################\n",
    "\n",
    "def gensig_euclidean(X, minlength=1, maxlength=None):\n",
    "    \"\"\"Generate the sigma for the squared difference from the mean.\"\"\"\n",
    "    cs = X.cumsum(0)\n",
    "    css = (X ** 2).sum(1).cumsum(0)\n",
    "\n",
    "    def sigma(i, j):\n",
    "        length = j - i\n",
    "        if minlength and length < minlength:\n",
    "            return np.inf\n",
    "        if maxlength and length > maxlength:\n",
    "            return np.inf\n",
    "        if i == 0:\n",
    "            return css[j - 1] - 1. / j * ((cs[j - 1]) ** 2).sum()\n",
    "        else:\n",
    "            return (css[j - 1] - css[i - 1]) - 1. / (j - i) * ((cs[j - 1] - cs[i - 1]) ** 2).sum()\n",
    "\n",
    "    return sigma\n",
    "\n",
    "def gensig_cosine(X, minlength=1, maxlength=None):\n",
    "    \"\"\"Generate the sigma for the cosine similarity.\"\"\"\n",
    "    def sigma(a, b):\n",
    "        length = b - a\n",
    "        if minlength and length < minlength:\n",
    "            return np.inf\n",
    "        if maxlength and length > maxlength:\n",
    "            return np.inf\n",
    "        rep = X[a:b].mean(0)\n",
    "        if length < 2:\n",
    "            return np.inf\n",
    "        return (cdist(X[a:b], [rep], 'cosine') ** 2).sum()\n",
    "\n",
    "    return sigma\n",
    "\n",
    "def gensig_model(X, minlength=1, maxlength=None, lam=0.0):\n",
    "    \"\"\"Generate the sigma for a model-based segmentation.\"\"\"\n",
    "    N, D = X.shape\n",
    "    over_sqrtD = 1. / np.sqrt(D)\n",
    "    cs = np.cumsum(X, 0)\n",
    "\n",
    "    def sigma(a, b):\n",
    "        length = b - a\n",
    "        if minlength and length < minlength:\n",
    "            return np.inf\n",
    "        if maxlength and length > maxlength:\n",
    "            return np.inf\n",
    "\n",
    "        tot = cs[b - 1].copy()\n",
    "        if a > 0:\n",
    "            tot -= cs[a - 1]\n",
    "        signs = np.sign(tot)\n",
    "        return -over_sqrtD * (signs * tot).sum()\n",
    "\n",
    "    return sigma\n",
    "\n",
    "def dpsplit(n, k, sig):\n",
    "    \"\"\"Perform the dynamic programming optimal segmentation.\"\"\"\n",
    "    K = k + 1\n",
    "    N = n\n",
    "    segtable = np.full((n, K), np.nan)\n",
    "    segtable[:, 0] = [sig(0, j + 1) for j in range(N)]\n",
    "    segindtable = np.full((N, K), -1, dtype='int')\n",
    "\n",
    "    for k in range(1, K):\n",
    "        for j in range(k, N):\n",
    "            ans = min(\n",
    "                ((segtable[l, k - 1] + sig(l + 1, j + 1), l + 1)\n",
    "                 for l in range(k - 1, j)), key=lambda x: x[0]\n",
    "            )\n",
    "            segtable[j, k] = ans[0]\n",
    "            segindtable[j, k] = ans[1]\n",
    "\n",
    "    current_pointer = segindtable[-1, K - 1]\n",
    "    path = [current_pointer]\n",
    "    for k in range(K - 2, 0, -1):\n",
    "        current_pointer = segindtable[current_pointer - 1, k]\n",
    "        path.append(current_pointer)\n",
    "\n",
    "    return sorted(path + [N]), segtable[-1, K - 1]\n",
    "\n",
    "####################\n",
    "# Greedy\n",
    "####################\n",
    "\n",
    "def greedysplit(n, k, sigma):\n",
    "    \"\"\"Perform a greedy split.\"\"\"\n",
    "    splits = [n]\n",
    "    s = sigma(0, n)\n",
    "\n",
    "    def score(splits, sigma):\n",
    "        splits = sorted(splits)\n",
    "        return sum(sigma(a, b) for (a, b) in seg_iter(splits))\n",
    "\n",
    "    while k > 0:\n",
    "        usedinds = set(splits)\n",
    "        new = min(\n",
    "            (score(splits + [i], sigma), splits + [i])\n",
    "            for i in range(1, n) if i not in usedinds\n",
    "        )\n",
    "        splits = new[1]\n",
    "        s = new[0]\n",
    "        k -= 1\n",
    "    return sorted(splits), s\n",
    "\n",
    "def refine(splits, sigma, n=1):\n",
    "    \"\"\"Refine splits a given number of steps.\"\"\"\n",
    "    oldsplits = splits[:]\n",
    "    counter = 0\n",
    "\n",
    "    while counter < n:\n",
    "        splits = [0] + splits\n",
    "        n = len(splits) - 2\n",
    "        new = [splits[0]]\n",
    "        for i in range(n):\n",
    "            out = bestsplit(splits[i], splits[i + 2], sigma)\n",
    "            new.append(out[2])\n",
    "        new.append(splits[-1])\n",
    "        splits = new[1:]\n",
    "\n",
    "        if splits == oldsplits:\n",
    "            break\n",
    "        oldsplits = splits[:]\n",
    "        counter += 1\n",
    "\n",
    "    return splits\n",
    "\n",
    "def bestsplit(low, high, sigma, minlength=1, maxlength=None):\n",
    "    \"\"\"Find the best split inside of a region.\"\"\"\n",
    "    length = high - low\n",
    "    if length < 2 * minlength:\n",
    "        return (np.inf, np.inf, low)\n",
    "    best = min(\n",
    "        ((sigma(low, j), sigma(j, high), j) for j in range(low + 1, high)),\n",
    "        key=lambda x: x[0] + x[1]\n",
    "    )\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l-Vi9gMZhn2Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article length: 239\n",
      "X length: 220\n",
      "Splitting...\n",
      "Initial Splits: [21, 38, 133, 175, 190, 220]\n",
      "Refining...\n",
      "Refined Splits: [24, 38, 133, 181, 190, 220]\n",
      "Printing refined splits... \n",
      "\n",
      "Segment: 0 Split Point: 24\n",
      "Context Before:\n",
      " START the quick brown fox jumps over the lazy dog . the dog , being lazy , just watched the fox jump . NL in a\n",
      "\n",
      "Context After:\n",
      " distant forest , animals of all kinds gathered under the tall , ancient trees . they were NL discussing the arrival of the new season . it was the time when the leaves turned golden , and NL the air became crisp . NL NL birds sang their songs , announcing that it was a season of change . the wise owl , sitting on NL a high branch , listened carefully to every voice . the deer were gathered near the stream , NL drinking the cool water . rabbits hopped around , playing in the fallen leaves ,\n",
      "\n",
      "Segment: 1 Split Point: 38\n",
      "Context Before:\n",
      " START the quick brown fox jumps over the lazy dog . the dog , being lazy , just watched the fox jump . NL in a distant forest , animals of all kinds gathered under the tall , ancient trees\n",
      "\n",
      "Context After:\n",
      " . they were NL discussing the arrival of the new season . it was the time when the leaves turned golden , and NL the air became crisp . NL NL birds sang their songs , announcing that it was a season of change . the wise owl , sitting on NL a high branch , listened carefully to every voice . the deer were gathered near the stream , NL drinking the cool water . rabbits hopped around , playing in the fallen leaves , and squirrels NL were busy gathering acorns for the upcoming winter . NL NL\n",
      "\n",
      "Segment: 2 Split Point: 133\n",
      "Context Before:\n",
      " discussing the arrival of the new season . it was the time when the leaves turned golden , and NL the air became crisp . NL NL birds sang their songs , announcing that it was a season of change . the wise owl , sitting on NL a high branch , listened carefully to every voice . the deer were gathered near the stream , NL drinking the cool water . rabbits hopped around , playing in the fallen leaves , and squirrels NL were busy gathering acorns for the upcoming winter . NL NL the forest was alive\n",
      "\n",
      "Context After:\n",
      " , full of chatter and activity . but even amidst all this liveliness , NL there was a sense of calm and harmony . every creature , big or small , had a role , and they NL performed it with grace . NL NL as the sun began to set , painting the sky in shades of orange and pink , the animals slowly NL returned to their nests , burrows , and dens . the forest fell quiet , awaiting the new adventures NL that the next day would bring . NL END\n",
      "\n",
      "Segment: 3 Split Point: 181\n",
      "Context Before:\n",
      " listened carefully to every voice . the deer were gathered near the stream , NL drinking the cool water . rabbits hopped around , playing in the fallen leaves , and squirrels NL were busy gathering acorns for the upcoming winter . NL NL the forest was alive , full of chatter and activity . but even amidst all this liveliness , NL there was a sense of calm and harmony . every creature , big or small , had a role , and they NL performed it with grace . NL NL as the sun began to set ,\n",
      "\n",
      "Context After:\n",
      " painting the sky in shades of orange and pink , the animals slowly NL returned to their nests , burrows , and dens . the forest fell quiet , awaiting the new adventures NL that the next day would bring . NL END\n",
      "\n",
      "Segment: 4 Split Point: 190\n",
      "Context Before:\n",
      " gathered near the stream , NL drinking the cool water . rabbits hopped around , playing in the fallen leaves , and squirrels NL were busy gathering acorns for the upcoming winter . NL NL the forest was alive , full of chatter and activity . but even amidst all this liveliness , NL there was a sense of calm and harmony . every creature , big or small , had a role , and they NL performed it with grace . NL NL as the sun began to set , painting the sky in shades of orange and pink\n",
      "\n",
      "Context After:\n",
      " , the animals slowly NL returned to their nests , burrows , and dens . the forest fell quiet , awaiting the new adventures NL that the next day would bring . NL END\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Make sure you provide arguments or set them directly\n",
    "# K = int(sys.argv[1]) if len(sys.argv) > 1 else 5  # Defaulting to 5 splits if no argument is provided\n",
    "# infile = sys.argv[2] if len(sys.argv) > 2 else \"sample.txt\"  # Default input file if not provided\n",
    "K = 5  # Default number of splits\n",
    "infile = \"sample.txt\"  # Default input file name\n",
    "with open(infile, \"r\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# Regular expressions to clean the text\n",
    "punctuation_pat = re.compile(r\"\"\"([!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~])\"\"\")\n",
    "hyphenline_pat = re.compile(r\"-\\s*\\n\\s*\")\n",
    "multiwhite_pat = re.compile(r\"\\s+\")\n",
    "cid_pat = re.compile(r\"\\(cid:\\d+\\)\")\n",
    "nonlet = re.compile(r\"([^A-Za-z0-9 ])\")\n",
    "\n",
    "def clean_text(txt):\n",
    "    # No need for utf-8 encode/decode in Python 3\n",
    "    txt = txt.lower()\n",
    "    txt = cid_pat.sub(\" UNK \", txt)\n",
    "    txt = hyphenline_pat.sub(\"\", txt)\n",
    "    txt = punctuation_pat.sub(r\" \\1 \", txt)\n",
    "    txt = re.sub(\"\\n\", \" NL \", txt)\n",
    "    txt = nonlet.sub(r\" \\1 \", txt)\n",
    "    txt = multiwhite_pat.sub(\" \", txt)\n",
    "    return \"\".join([\"START \", txt.strip(), \" END\"])\n",
    "\n",
    "txt = clean_text(txt).split()\n",
    "\n",
    "# Load vectors and vocabulary\n",
    "vecs = np.load(\"data/vecs.npy\")\n",
    "words = np.load(\"data/vocab.npy\", allow_pickle=True)\n",
    "word_lookup = {w: c for c, w in enumerate(words)}\n",
    "\n",
    "print(\"Article length:\", len(txt))\n",
    "\n",
    "X = []\n",
    "\n",
    "mapper = {}\n",
    "count = 0\n",
    "for i, word in enumerate(txt):\n",
    "    if word in word_lookup:\n",
    "        mapper[i] = count\n",
    "        count += 1\n",
    "        X.append(vecs[word_lookup[word]])\n",
    "\n",
    "# Reverse mapper\n",
    "mapperr = {v: k for k, v in mapper.items()}\n",
    "\n",
    "X = np.array(X)\n",
    "print(\"X length:\", X.shape[0])\n",
    "\n",
    "# Generate segmentation using greedy split\n",
    "sig = gensig_model(X)\n",
    "print(\"Splitting...\")\n",
    "splits, e = greedysplit(X.shape[0], K, sig)\n",
    "print(\"Initial Splits:\", splits)\n",
    "\n",
    "# Refine the splits\n",
    "print(\"Refining...\")\n",
    "splitsr = refine(splits, sig, 20)\n",
    "print(\"Refined Splits:\", splitsr)\n",
    "\n",
    "# Print refined splits with surrounding text for context\n",
    "print(\"Printing refined splits... \")\n",
    "\n",
    "for i, s in enumerate(splitsr[:-1]):\n",
    "    k = mapperr.get(s, len(txt))\n",
    "    print(\"\\nSegment:\", i, \"Split Point:\", s)\n",
    "    print(\"Context Before:\\n\", \" \".join(txt[max(0, k - 100):k]))\n",
    "    print(\"\\nContext After:\\n\", \" \".join(txt[k:k + 100]))\n",
    "\n",
    "# Save results to a file\n",
    "with open(f\"result_{K}.txt\", \"w\") as f:\n",
    "    prev = 0\n",
    "    for s in splitsr:\n",
    "        k = mapperr.get(s, len(txt))\n",
    "        f.write(\" \".join(txt[prev:k]).replace(\"NL\", \"\\n\"))\n",
    "        f.write(\"\\nBREAK\\n\")\n",
    "        prev = k\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "GloVe embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "words = np.load(\"data/vocab.npy\", allow_pickle=True)\n",
    "vecs = np.load(\"data/vecs.npy\")\n",
    "print(\"GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# Create word lookup dictionary\n",
    "word_lookup = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "# Function to transform sentences using tf-idf weighting\n",
    "def tfidf_transform(vectors):\n",
    "    transformer = TfidfTransformer()\n",
    "    return transformer.fit_transform(vectors).toarray()\n",
    "\n",
    "# Function to cluster GloVe embeddings using K-means for C99k50, C99k200\n",
    "def get_clustered_vectors(word_vectors, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(word_vectors)\n",
    "    return kmeans.cluster_centers_, kmeans.predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oC99 (Choi 3-5)\n",
      "P_k: 14.10, WD: 14.10\n",
      "oC99tf (Choi 3-5)\n",
      "P_k: 13.63, WD: 13.63\n",
      "oC99tfidf (Choi 3-5)\n",
      "P_k: 13.23, WD: 13.23\n"
     ]
    }
   ],
   "source": [
    "# Load Choi dataset (3-5 set only)\n",
    "choi_documents = list(allchoi(\"3-5\"))\n",
    "\n",
    "### Configuration 1: oC99 (cosine similarity without tf-idf or clustering)\n",
    "print(\"oC99 (Choi 3-5)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vec_sents(doc, word_lookup, vecs)  # Direct cosine-based vectors\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 8, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 2: oC99tf (tf-based without idf weighting)\n",
    "print(\"oC99tf (Choi 3-5)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 3: oC99tfidf (tf-idf with idf weighting)\n",
    "print(\"oC99tfidf (Choi 3-5)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    vectors = tfidf_transform(vectors)  # Apply tf-idf weighting\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oC99k50 (Choi 3-5)\n"
     ]
    }
   ],
   "source": [
    "### Configuration 4: oC99k50 (cosine similarity with KMeans clustering, 50 clusters)\n",
    "print(\"oC99k50 (Choi 3-5)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "clustered_centers, word_labels = get_clustered_vectors(vecs, n_clusters=50)\n",
    "for doc in choi_documents:\n",
    "    clustered_vectors = np.array([\n",
    "        clustered_centers[word_labels[word_lookup[word]]]\n",
    "        for part in doc for sent in part for word in sent if word in word_lookup\n",
    "    ])\n",
    "    clustered_vectors = clustered_vectors.reshape(len(doc), -1)\n",
    "    sig = gensig_model(clustered_vectors)\n",
    "    splits, _ = greedysplit(clustered_vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 5: oC99k200 (cosine similarity with KMeans clustering, 200 clusters)\n",
    "print(\"oC99k200 (Choi 3-5)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "clustered_centers, word_labels = get_clustered_vectors(vecs, n_clusters=200)\n",
    "for doc in choi_documents:\n",
    "    clustered_vectors = np.array([\n",
    "        clustered_centers[word_labels[word_lookup[word]]]\n",
    "        for part in doc for sent in part for word in sent if word in word_lookup\n",
    "    ])\n",
    "    clustered_vectors = clustered_vectors.reshape(len(doc), -1)\n",
    "    sig = gensig_model(clustered_vectors)\n",
    "    splits, _ = greedysplit(clustered_vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oC99 (Choi 6-8)\n",
      "P_k: 14.40, WD: 16.24\n",
      "oC99tf (Choi 6-8)\n",
      "P_k: 14.40, WD: 16.24\n",
      "oC99tfidf (Choi 6-8)\n",
      "P_k: 14.42, WD: 16.42\n"
     ]
    }
   ],
   "source": [
    "# Load Choi dataset (6-8 set only)\n",
    "choi_documents = list(allchoi(\"6-8\"))\n",
    "\n",
    "### Configuration 1: oC99 (cosine similarity without tf-idf or clustering)\n",
    "print(\"oC99 (Choi 6-8)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vec_sents(doc, word_lookup, vecs)  # Direct cosine-based vectors\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 10, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 2: oC99tf (tf-based without idf weighting)\n",
    "print(\"oC99tf (Choi 6-8)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 10, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 3: oC99tfidf (tf-idf with idf weighting)\n",
    "print(\"oC99tfidf (Choi 6-8)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    vectors = tfidf_transform(vectors)  # Apply tf-idf weighting\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 10, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oC99 (Choi 9-11)\n",
      "P_k: 13.81, WD: 16.12\n",
      "oC99tf (Choi 9-11)\n",
      "P_k: 13.50, WD: 15.12\n",
      "oC99tfidf (Choi 9-11)\n",
      "P_k: 14.54, WD: 16.35\n"
     ]
    }
   ],
   "source": [
    "# Load Choi dataset (9-11 set only)\n",
    "choi_documents = list(allchoi(\"9-11\"))\n",
    "\n",
    "### Configuration 1: oC99 (cosine similarity without tf-idf or clustering)\n",
    "print(\"oC99 (Choi 9-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vec_sents(doc, word_lookup, vecs)  # Direct cosine-based vectors\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 10, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 2: oC99tf (tf-based without idf weighting)\n",
    "print(\"oC99tf (Choi 9-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 3: oC99tfidf (tf-idf with idf weighting)\n",
    "print(\"oC99tfidf (Choi 9-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    vectors = tfidf_transform(vectors)  # Apply tf-idf weighting\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oC99 (Choi 3-11)\n",
      "P_k: 15.00, WD: 16.52\n",
      "oC99tf (Choi 3-11)\n",
      "P_k: 15.00, WD: 16.52\n",
      "oC99tfidf (Choi 3-11)\n",
      "P_k: 15.84, WD: 17.47\n"
     ]
    }
   ],
   "source": [
    "# Load Choi dataset (3-11)\n",
    "choi_documents = list(allchoi(\"3-11\"))\n",
    "\n",
    "### Configuration 1: oC99 (cosine similarity without tf-idf or clustering)\n",
    "print(\"oC99 (Choi 3-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vec_sents(doc, word_lookup, vecs)  # Direct cosine-based vectors\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 2: oC99tf (tf-based without idf weighting)\n",
    "print(\"oC99tf (Choi 3-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")\n",
    "\n",
    "### Configuration 3: oC99tfidf (tf-idf with idf weighting)\n",
    "print(\"oC99tfidf (Choi 3-11)\")\n",
    "pk_scores, wd_scores = [], []\n",
    "for doc in choi_documents:\n",
    "    vectors = vecdf_sents(doc, word_lookup, vecs, dfcounter={})  # tf-based representation\n",
    "    vectors = tfidf_transform(vectors)  # Apply tf-idf weighting\n",
    "    sig = gensig_model(vectors)\n",
    "    splits, _ = greedysplit(vectors.shape[0], 9, sig)\n",
    "    ref_splits = refsplit_sent(doc)\n",
    "    pk_scores.append(score(splits, ref_splits))\n",
    "    wd_scores.append(score_wd(splits, ref_splits))\n",
    "P_k = np.mean(pk_scores) * 100\n",
    "WD = np.mean(wd_scores) * 100\n",
    "print(f\"P_k: {P_k:.2f}, WD: {WD:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "GloVe embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "# Load GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "words = np.load(\"vocab1.npy\", allow_pickle=True)\n",
    "vecs = np.load(\"vecs1.npy\")\n",
    "word_lookup = {word: idx for idx, word in enumerate(words)}\n",
    "print(\"GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# Dataset directory\n",
    "arx_directory = \"./Github Code/data/arxiv/\"\n",
    "    \n",
    "# Helper functions for vector normalization\n",
    "def normalize_vectors(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / norms\n",
    "\n",
    "# Function to get document vector representation\n",
    "def get_document_vectors(doc, method=\"oC99\"):\n",
    "    #print(f\"Generating document vectors for method: {method}\")\n",
    "    if method == \"oC99\":\n",
    "        # C99 with word embeddings, no normalization\n",
    "        return vec_sents(doc, word_lookup, vecs)\n",
    "    elif method == \"CVS\":\n",
    "        # CVS with cosine similarity\n",
    "        return vecdf_sents(doc, word_lookup, vecs, dfcounter=dfcounter)\n",
    "    elif method == \"CVSn\":\n",
    "        # CVS with normalized word embeddings\n",
    "        normalized_vecs = normalize_vectors(vecs)\n",
    "        return vecdf_sents(doc, word_lookup, normalized_vecs, dfcounter=dfcounter)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 documents from the Arxiv dataset.\n",
      "Document frequency counter built with 20397 unique words.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Function to build the document frequency counter (dfcounter)\n",
    "def build_dfcounter(documents):\n",
    "    dfcounter = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        # Track unique words in this document only\n",
    "        words_in_doc = set(word for part in doc for sent in part for word in sent)\n",
    "        for word in words_in_doc:\n",
    "            dfcounter[word] += 1\n",
    "    return dfcounter\n",
    "\n",
    "# Load the Arxiv dataset documents\n",
    "def arx_loader(file_path):\n",
    "    \"\"\" Load an arxiv document from the dataset,\n",
    "        returns a list of parts\n",
    "        each part is a list of words \"\"\"\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            doc = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File {file_path} not found. Please check the file paths.\")\n",
    "\n",
    "    return [[[x] for x in x.split()] for x in doc.split(\"BR\")]\n",
    "\n",
    "# Directory and file loading setup\n",
    "arx_directory = \"./Github Code/data/arxiv/\"\n",
    "arx_files = [f for f in os.listdir(arx_directory) if f.endswith('.ref')]\n",
    "\n",
    "# Process and load each document with arx_loader (first 100 files only)\n",
    "arx_documents = [arx_loader(os.path.join(arx_directory, file_name)) for file_name in arx_files[:100]]\n",
    "print(f\"Loaded {len(arx_documents)} documents from the Arxiv dataset.\")\n",
    "\n",
    "# Build the document frequency counter (dfcounter)\n",
    "dfcounter = build_dfcounter(arx_documents)\n",
    "print(f\"Document frequency counter built with {len(dfcounter)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation(documents, num_splits, strategy='G', reference_method=None, word_lookup=None, word_reps=None, method='oC99', num_iter=20):\n",
    "    \"\"\"\n",
    "    Evaluate the segmentation of a list of documents using the P_k and WindowDiff metrics.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): List of documents to segment.\n",
    "    num_splits (int): Number of splits to perform.\n",
    "    strategy (str): Strategy to use for generating splits. 'G' for Greedy, 'R' for Refined.\n",
    "    reference_method (function): Function to generate reference splits for each document.\n",
    "    word_lookup (dict): Dictionary mapping words to their indices in the word representations.\n",
    "    word_reps (np.ndarray): Matrix of word representations (e.g., GloVe embeddings).\n",
    "    method (str): Method to use for generating document vectors, e.g., 'oC99', 'CVS'.\n",
    "    num_iter (int): Number of refinement iterations, if strategy is 'R'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Average P_k and WD scores (multiplied by 100 for percentage).\n",
    "    \"\"\"\n",
    "    p_k_scores = []\n",
    "    wd_scores = []\n",
    "\n",
    "    print(f\"Evaluating {method} with strategy {strategy}\")\n",
    "    for doc in documents:\n",
    "        # Generate document vectors based on the specified method\n",
    "        X = get_document_vectors(doc, method=method)\n",
    "\n",
    "        # Generate hypothesis splits\n",
    "        sig = gensig_model(X)\n",
    "        if strategy == 'G':\n",
    "            hyp_splits, _ = greedysplit(X.shape[0], num_splits, sig)\n",
    "        elif strategy == 'R':\n",
    "            initial_splits, _ = greedysplit(X.shape[0], num_splits, sig)\n",
    "            hyp_splits = refine(initial_splits, sig, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid strategy. Use 'G' for Greedy or 'R' for Refined.\")\n",
    "\n",
    "        # Generate reference splits\n",
    "        ref_splits = reference_method(doc)\n",
    "\n",
    "        # Calculate P_k and WindowDiff scores\n",
    "        p_k = score(hyp_splits, ref_splits)\n",
    "        wd = score_wd(hyp_splits, ref_splits)\n",
    "\n",
    "        # Append scores for averaging\n",
    "        p_k_scores.append(p_k)\n",
    "        wd_scores.append(wd)\n",
    "\n",
    "    # Calculate average P_k and WD scores\n",
    "    avg_p_k = np.mean(p_k_scores) * 100\n",
    "    avg_wd = np.mean(wd_scores) * 100\n",
    "\n",
    "    print(f'{method} - {strategy}: P_k: {avg_p_k:.2f}, WD: {avg_wd:.2f}')\n",
    "    print()\n",
    "\n",
    "    return avg_p_k, avg_wd\n",
    "\n",
    "# Set up parameters\n",
    "word_lookup = {word: idx for idx, word in enumerate(words)}\n",
    "word_reps = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For X-archive dataset\n",
      "Evaluating oC99 with strategy G\n",
      "oC99 - G: P_k: 40.56, WD: 40.57\n",
      "\n",
      "Evaluating oC99 with strategy R\n",
      "oC99 - R: P_k: 40.57, WD: 40.60\n",
      "\n",
      "Evaluating CVS with strategy G\n",
      "CVS - G: P_k: 30.26, WD: 31.61\n",
      "\n",
      "Evaluating CVS with strategy R\n",
      "CVS - R: P_k: 30.15, WD: 31.54\n",
      "\n",
      "Evaluating CVSn with strategy G\n",
      "CVSn - G: P_k: 28.21, WD: 29.96\n",
      "\n",
      "Evaluating CVSn with strategy R\n",
      "CVSn - R: P_k: 28.48, WD: 30.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For X-archive dataset\")\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=2, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=2, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=7, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=7, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=8, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    arx_documents, \n",
    "    num_splits=8, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_documents = list(allchoi(\"3-5\"))\n",
    "\n",
    "print(\"For Choi dataset (3-5)\")\n",
    "\n",
    "# Build the document frequency counter (dfcounter)\n",
    "dfcounter = build_dfcounter(choi_documents)\n",
    "#print(f\"Document frequency counter built with {len(dfcounter)} unique words.\")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Choi dataset (6-8)\n",
      "Evaluating oC99 with strategy G\n",
      "oC99 - G: P_k: 21.43, WD: 21.75\n",
      "\n",
      "Evaluating oC99 with strategy R\n",
      "oC99 - R: P_k: 20.59, WD: 20.90\n",
      "\n",
      "Evaluating CVS with strategy G\n",
      "CVS - G: P_k: 13.22, WD: 13.49\n",
      "\n",
      "Evaluating CVS with strategy R\n",
      "CVS - R: P_k: 12.54, WD: 12.80\n",
      "\n",
      "Evaluating CVSn with strategy G\n",
      "CVSn - G: P_k: 11.25, WD: 11.71\n",
      "\n",
      "Evaluating CVSn with strategy R\n",
      "CVSn - R: P_k: 9.91, WD: 10.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choi_documents = list(allchoi(\"6-8\"))\n",
    "\n",
    "print(\"For Choi dataset (6-8)\")\n",
    "\n",
    "# Build the document frequency counter (dfcounter)\n",
    "dfcounter = build_dfcounter(choi_documents)\n",
    "#print(f\"Document frequency counter built with {len(dfcounter)} unique words.\")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_documents = list(allchoi(\"9-11\"))\n",
    "\n",
    "print(\"For Choi dataset (9-11)\")\n",
    "\n",
    "# Build the document frequency counter (dfcounter)\n",
    "dfcounter = build_dfcounter(choi_documents)\n",
    "#print(f\"Document frequency counter built with {len(dfcounter)} unique words.\")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Choi dataset (3-11)\n",
      "Evaluating oC99 with strategy G\n",
      "oC99 - G: P_k: 21.17, WD: 21.62\n",
      "\n",
      "Evaluating oC99 with strategy R\n",
      "oC99 - R: P_k: 20.33, WD: 20.74\n",
      "\n",
      "Evaluating CVS with strategy G\n",
      "CVS - G: P_k: 12.67, WD: 12.88\n",
      "\n",
      "Evaluating CVS with strategy R\n",
      "CVS - R: P_k: 12.22, WD: 12.39\n",
      "\n",
      "Evaluating CVSn with strategy G\n",
      "CVSn - G: P_k: 11.09, WD: 11.45\n",
      "\n",
      "Evaluating CVSn with strategy R\n",
      "CVSn - R: P_k: 10.38, WD: 10.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choi_documents = list(allchoi(\"3-11\"))\n",
    "\n",
    "print(\"For Choi dataset (3-11)\")\n",
    "\n",
    "# Build the document frequency counter (dfcounter)\n",
    "dfcounter = build_dfcounter(choi_documents)\n",
    "#print(f\"Document frequency counter built with {len(dfcounter)} unique words.\")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=5, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='oC99', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=7, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVS', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='G', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")\n",
    "\n",
    "pk_score, wd_score = evaluate_segmentation(\n",
    "    choi_documents, \n",
    "    num_splits=8, \n",
    "    strategy='R', \n",
    "    reference_method=refsplit_sent, \n",
    "    word_lookup=word_lookup, \n",
    "    word_reps=word_reps, \n",
    "    method='CVSn', \n",
    "    num_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
